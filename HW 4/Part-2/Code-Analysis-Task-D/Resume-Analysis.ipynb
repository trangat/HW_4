{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Analysis\n",
    "_**HARD: This is a curveball assignment. Plus, this is Python without Pandas.**_\n",
    "\n",
    "#### The objective of this assignment is for you to explain what is happening in each cell in clear, understandable language. \n",
    "\n",
    "#### _There is no need to code._ The code is there for you, and it already runs. Your task is only to explain what each line in each cell does.\n",
    "\n",
    "#### The placeholder cells should describe what happens in the cell below it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below imports `os` as a dependency because the `os.path.join` function. Also, the `string` dependency is needed because later in the script, `string.punctuation` will be used to detect and remove punctuation symbols. Explain what the line `from collection import Counter` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below allows us to find the correct path to access the md file that holds the data. In this case, the md in question is located in the same directory.\n",
    "Then we create a dictionary for Required Skills and Desired Skills that we'd like to match to the resume. They are in all caps because we are creating a set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "resume_path = os.path.join(\".\", 'resume.md')\n",
    "\n",
    "# Skills to match\n",
    "REQUIRED_SKILLS = {\"excel\", \"python\", \"mysql\", \"statistics\"}\n",
    "DESIRED_SKILLS = {\"r\", \"git\", \"html\", \"css\", \"leaflet\", \"modeling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function we define below opens and reads the resume and its contents. It splits its contents on the resume 'tokens,' which are just word elements of the resume. It returns these resume_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filepath):\n",
    "    # Helper function to read a file and return the data.\n",
    "    with open(filepath, \"r\") as resume_file_handler:\n",
    "        resume_contents = resume_file_handler.read()\n",
    "        resume_contents = resume_contents.lower()\n",
    "        resume_tokens = resume_contents.split()\n",
    "        return resume_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is using the above function we defined to read resume_path (the file path we defined above) and attributes this to 'word_list.' 'Word_list' stores this information and we will be able to use it to find out information later regarding what skills this resume in question lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the text for a Resume\n",
    "word_list = load_file(resume_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set is created to hold all the unique words in the resume so we can see whether or not this resume has any of the desired/required skills. We populate the set with the resume tokens (the words) in word_list. The punctuation set will allow us to remove the punctuation so we get a clear picture of the actual words in the resume. We then re-establish 'resume' by subtracting the punctuation set from the resume set. Then we print out the list of words to check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WORDS BEFORE MOVING PUNCTUATION\n",
      "{'javascript,', 'advanced', 'forecasting', 'in', 'scripts', '*', 'to', 'd3', 'mining,', 'pivot', 'learning', 'python,', 'social', 'interests', 'r,', 'skills', 'vba', 'd3,', 'creating', 'mysql', 'big', 'with', 'from', 'the', 'aws', 'camp', 'basic', 'stein', 'business', 'analytics', 'writing', 'tables', 'databases', 'machine', '##', 'excel,', 'mongodb', 'education', 'analyze', 'frank', 'statistics', 'front-end', 'mining', 'visualization', 'performing', 'sets', 'developing', 'hadoop,', 'html,', 'git/github', 'apis.', 'open-source', 'interactions,', 'graduate', 'media', 'experience', 'bootstrap,', 'web', 'algorithms', 'sql,', 'modeling', 'files', 'visualizations', 'cloud', 'tableau', 'apps', 'learning,', '#', 'using', 'working', 'leaflet.js', 'microsoft', 'software', 'n.', 'and', 'excel.', 'hadoop', 'html/css,', 'tableau,', 'boot', 'pandas', 'designing', 'statistics,', 'contributing', 'python', 'intelligence', 'api', 'css,', 'data'}\n",
      "\n",
      "WORDS AFTER MOVING PUNCTUATION\n",
      "{'javascript,', 'advanced', 'forecasting', 'in', 'scripts', 'to', 'd3', 'mining,', 'pivot', 'learning', 'python,', 'social', 'interests', 'r,', 'skills', 'vba', 'd3,', 'creating', 'mysql', 'big', 'with', 'from', 'the', 'aws', 'camp', 'basic', 'stein', 'business', 'analytics', 'writing', 'tables', 'databases', 'machine', '##', 'excel,', 'mongodb', 'education', 'analyze', 'frank', 'statistics', 'front-end', 'mining', 'visualization', 'performing', 'sets', 'developing', 'hadoop,', 'html,', 'git/github', 'apis.', 'open-source', 'interactions,', 'graduate', 'media', 'experience', 'bootstrap,', 'web', 'algorithms', 'sql,', 'modeling', 'files', 'visualizations', 'cloud', 'tableau', 'apps', 'learning,', 'using', 'working', 'leaflet.js', 'microsoft', 'software', 'n.', 'and', 'excel.', 'hadoop', 'html/css,', 'tableau,', 'boot', 'pandas', 'designing', 'statistics,', 'contributing', 'python', 'intelligence', 'api', 'css,', 'data'}\n"
     ]
    }
   ],
   "source": [
    "# Create a set of unique words from the resume\n",
    "resume = set()\n",
    "\n",
    "# HINT: Single elements in a programming language are often referred to as tokens\n",
    "for token in word_list:\n",
    "    resume.add(token)\n",
    "\n",
    "print('\\nWORDS BEFORE MOVING PUNCTUATION')    \n",
    "print(resume)\n",
    "\n",
    "# Remove Punctuation that were read as whole words\n",
    "punctuation = set(string.punctuation)\n",
    "# HINT: Attributes that are in `resume` that are not in `punctuation` (difference)\n",
    "resume = resume - punctuation\n",
    "\n",
    "print('\\nWORDS AFTER MOVING PUNCTUATION')    \n",
    "print(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set intersection allows us to cross-reference the words in 'resume' and the 'required_skills.' The same thing occurs with 'resume' and 'desired_skills.' We then print the words that they share in common only. The character cleaning function cleans up characters (## and n.), and it only leaves full words. The word punctuation cleaning keeps those characters still in the list. Yes, you can add more items to the list if you needed to remove them. The word_list list comprehension prints the words in the word_list as long as they are not present in stop_words list. It prints only the words that have no punctuation, are only strings, and not in stop_words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REQUIRED SKILLS\n",
      "{'mysql', 'statistics', 'python'}\n",
      "DESIRED SKILLS\n",
      "{'modeling'}\n",
      "\n",
      "WORD LIST AFTER PUNCTUATION REMOVAL\n",
      "['frank', 'n.', 'stein', '##', 'education', 'data', 'analytics', 'and', 'visualization', 'boot', 'camp', 'graduate', '##', 'experience', 'creating', 'pivot', 'tables', 'and', 'vba', 'scripts', 'in', 'excel.', 'modeling', 'and', 'forecasting', 'data', 'using', 'basic', 'statistics', 'writing', 'python', 'scripts', 'to', 'analyze', 'data', 'sets', 'from', 'files', 'and', 'apis.', 'social', 'media', 'mining', 'using', 'python', 'working', 'with', 'mysql', 'and', 'mongodb', 'databases', 'developing', 'front-end', 'web', 'visualizations', 'using', 'html,', 'css,', 'bootstrap,', 'd3,', 'and', 'leaflet.js', 'using', 'the', 'tableau', 'business', 'intelligence', 'software', 'performing', 'big', 'data', 'analytics', 'with', 'hadoop', 'working', 'with', 'machine', 'learning', 'algorithms', '##', 'skills', 'microsoft', 'excel,', 'python,', 'javascript,', 'html/css,', 'api', 'interactions,', 'social', 'media', 'mining,', 'sql,', 'hadoop,', 'tableau,', 'advanced', 'statistics,', 'machine', 'learning,', 'r,', 'git/github', '##', 'interests', 'contributing', 'to', 'open-source', 'software', 'data', 'analytics', 'with', 'python', 'and', 'pandas', 'designing', 'data', 'visualization', 'web', 'apps', 'with', 'html,', 'css,', 'javascript,', 'and', 'd3', 'working', 'with', 'big', 'data', 'in', 'the', 'cloud', 'using', 'aws']\n",
      "\n",
      "WORD LIST AFTER CHARACTER PUNCTUATION REMOVAL\n",
      "['frank', 'n', 'stein', '', 'education', 'data', 'analytics', 'and', 'visualization', 'boot', 'camp', 'graduate', '', 'experience', 'creating', 'pivot', 'tables', 'and', 'vba', 'scripts', 'in', 'excel', 'modeling', 'and', 'forecasting', 'data', 'using', 'basic', 'statistics', 'writing', 'python', 'scripts', 'to', 'analyze', 'data', 'sets', 'from', 'files', 'and', 'apis', 'social', 'media', 'mining', 'using', 'python', 'working', 'with', 'mysql', 'and', 'mongodb', 'databases', 'developing', 'frontend', 'web', 'visualizations', 'using', 'html', 'css', 'bootstrap', 'd3', 'and', 'leafletjs', 'using', 'the', 'tableau', 'business', 'intelligence', 'software', 'performing', 'big', 'data', 'analytics', 'with', 'hadoop', 'working', 'with', 'machine', 'learning', 'algorithms', '', 'skills', 'microsoft', 'excel', 'python', 'javascript', 'htmlcss', 'api', 'interactions', 'social', 'media', 'mining', 'sql', 'hadoop', 'tableau', 'advanced', 'statistics', 'machine', 'learning', 'r', 'gitgithub', '', 'interests', 'contributing', 'to', 'opensource', 'software', 'data', 'analytics', 'with', 'python', 'and', 'pandas', 'designing', 'data', 'visualization', 'web', 'apps', 'with', 'html', 'css', 'javascript', 'and', 'd3', 'working', 'with', 'big', 'data', 'in', 'the', 'cloud', 'using', 'aws']\n",
      "\n",
      "WORD LIST AFTER STOP WORDS\n",
      "['frank', 'n', 'stein', '', 'education', 'data', 'analytics', 'visualization', 'boot', 'camp', 'graduate', '', 'experience', 'creating', 'pivot', 'tables', 'vba', 'scripts', 'excel', 'modeling', 'forecasting', 'data', 'basic', 'statistics', 'writing', 'python', 'scripts', 'analyze', 'data', 'sets', 'from', 'files', 'apis', 'social', 'media', 'mining', 'python', 'mysql', 'mongodb', 'databases', 'developing', 'frontend', 'web', 'visualizations', 'html', 'css', 'bootstrap', 'd3', 'leafletjs', 'the', 'tableau', 'business', 'intelligence', 'software', 'performing', 'big', 'data', 'analytics', 'hadoop', 'machine', 'learning', 'algorithms', '', 'skills', 'microsoft', 'excel', 'python', 'javascript', 'htmlcss', 'api', 'interactions', 'social', 'media', 'mining', 'sql', 'hadoop', 'tableau', 'advanced', 'statistics', 'machine', 'learning', 'r', 'gitgithub', '', 'interests', 'contributing', 'opensource', 'software', 'data', 'analytics', 'python', 'pandas', 'designing', 'data', 'visualization', 'web', 'apps', 'html', 'css', 'javascript', 'd3', 'big', 'data', 'the', 'cloud', 'aws']\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Required Skills Match using Set Intersection\n",
    "print('REQUIRED SKILLS')\n",
    "print(resume & REQUIRED_SKILLS)\n",
    "\n",
    "# Calculate the Desired Skills Match using Set Intersection\n",
    "print('DESIRED SKILLS')\n",
    "print(resume & DESIRED_SKILLS)\n",
    "\n",
    "\n",
    "# Word Punctuation Cleaning\n",
    "word_list = [word for word in word_list if word not in string.punctuation]\n",
    "print('\\nWORD LIST AFTER PUNCTUATION REMOVAL')\n",
    "print(word_list)\n",
    "\n",
    "# Character Punctuation Cleaning\n",
    "word_list = [''.join(char for char in word if char not in string.punctuation) for word in word_list]\n",
    "print('\\nWORD LIST AFTER CHARACTER PUNCTUATION REMOVAL')\n",
    "print(word_list)\n",
    "\n",
    "# Clean Stop Words\n",
    "stop_words = [\"and\", \"with\", \"using\", \"##\", \"working\", \"in\", \"to\"]\n",
    "word_list = [word for word in word_list if word not in stop_words]\n",
    "print('\\nWORD LIST AFTER STOP WORDS')\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word_count dictionary was initialized with values that equal 0, and the keys are the words in the word_list we cleaned in the cell above. The loop iterates through each of the words (keys) in the word_list from above. The word_count increases by 1 if the same word is found. The counter does the same thing, but instead of a loop, we imported counter in the very beginning, and it will simply count the unique words present in word_list. We then print Top 10 Words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Top 10 Words\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "# Resume Word Count\n",
    "# ==========================\n",
    "# Initialize a dictionary with default values equal to zero\n",
    "word_count = {}.fromkeys(word_list, 0)\n",
    "\n",
    "# Loop through the word list and count each word.\n",
    "for word in word_list:\n",
    "    word_count[word] += 1\n",
    "# print(word_count)\n",
    "\n",
    "# Bonus using collections.Counter\n",
    "word_counter = Counter(word_list)\n",
    "# print(word_counter)\n",
    "\n",
    "# Comparing both word count solutions\n",
    "print(word_count == word_counter)\n",
    "\n",
    "# Top 10 Words\n",
    "print(\"Top 10 Words\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cleate a set for sorted_words. Then we write a for loop to create a list with the Top 10 most frequently appearing words in word_list. The :10 means that the the list will stop at the tenth most frequent word, printing only the top words and their frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: data                 Count: 7\n",
      "Token:                      Count: 4\n",
      "Token: python               Count: 4\n",
      "Token: analytics            Count: 3\n",
      "Token: visualization        Count: 2\n",
      "Token: scripts              Count: 2\n",
      "Token: excel                Count: 2\n",
      "Token: statistics           Count: 2\n",
      "Token: social               Count: 2\n",
      "Token: media                Count: 2\n"
     ]
    }
   ],
   "source": [
    "# Sort words by count and print the top 10\n",
    "sorted_words = []\n",
    "for word in sorted(word_count, key=word_count.get, reverse=True)[:10]:\n",
    "    print(f\"Token: {word:20} Count: {word_count[word]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
